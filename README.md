# Work
I'm currently on sabbatical, and am starting to explore some early stage ideas in both applied interpretability and foundation datasets.

I was at DeepMind from 2019 to late 2023, where I worked on:
- Interpretability for LLMs (e.g. the [Hydra Effect](https://arxiv.org/abs/2307.15771), [Copy Suppression](https://arxiv.org/abs/2310.04625)) and [AlphaZero](https://www.pnas.org/doi/full/10.1073/pnas.2206625119).
- Science of training data.
- RLHF data quality and self-annotation.
- Evaluation of generalist deep RL agents.

I did my PhD ([thesis](https://spiral.imperial.ac.uk/bitstream/10044/1/85919/1/McGrath-T-2020-PhD-Thesis.pdf)) at Imperial College with [Nick Jones](https://www.imperial.ac.uk/people/nick.jones/research.html) and [Kevin Murphy](https://www.imperial.ac.uk/people/k.g.murphy).

# Research
My papers are listed on my [Google Scholar page](https://scholar.google.com/citations?user=wZurn8MAAAAJ&hl=en). I have a list of [research projects I'm interested in working on](https://tommcgrath.github.io/research-projects).

If there's something on there you're interested in collaborating on, please get in touch!

# Writing
I have a [substack](https://banburismus.substack.com/) if you prefer to read there.
- [Safety as a scientific pursuit](https://tommcgrath.github.io/safety_as_science)

# Contact me
[Email](mailto:thomas.m.mcgrath@gmail.com) is probably best, but you can reach me on [Twitter](https://twitter.com/banburismus_) or [LinkedIn](https://www.linkedin.com/in/tom-mcgrath-7337bb151/) as well.
